from flask import Flask, request, jsonify, render_template
import joblib
import pefile
import pandas as pd
from sklearn.preprocessing import StandardScaler

app = Flask(__name__)

# Load the model at the start of the app
model = joblib.load('random_forest_model_rfe.pkl')

import math
import pefile
import re

def calculate_entropy(data):
    if not data:
        return 0.0
    entropy = 0
    for x in range(256):
        p_x = float(data.count(bytes([x]))) / len(data)
        if p_x > 0:
            entropy += - p_x*math.log(p_x, 2)
    return entropy

def extract_features(file):
    try:
        # Load the PE file
        pe = pefile.PE(file)
    except Exception as e:
        print(f"Failed to load the PE file: {e}")
        return None

    # Extract features
    features = {}

    try:
        features['Characteristics'] = pe.FILE_HEADER.Characteristics
        features['MajorLinkerVersion'] = pe.OPTIONAL_HEADER.MajorLinkerVersion
        features['AddressOfEntryPoint'] = pe.OPTIONAL_HEADER.AddressOfEntryPoint
        features['BaseOfData'] = pe.OPTIONAL_HEADER.BaseOfData
        features['MajorOperatingSystemVersion'] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion
        features['MinorImageVersion'] = pe.OPTIONAL_HEADER.MinorImageVersion
        features['CheckSum'] = pe.OPTIONAL_HEADER.CheckSum
        features['Subsystem'] = pe.OPTIONAL_HEADER.Subsystem
        features['DllCharacteristics'] = pe.OPTIONAL_HEADER.DllCharacteristics
        features['SizeOfStackReserve'] = pe.OPTIONAL_HEADER.SizeOfStackReserve
    except Exception as e:
        print(f"Failed to extract header features: {e}")

    try:
        features['SectionsMinEntropy'] = min((calculate_entropy(section.get_data()) for section in pe.sections), default=0)
        features['SectionsMaxEntropy'] = max((calculate_entropy(section.get_data()) for section in pe.sections), default=0)
        features['SectionsMinRawsize'] = min((section.SizeOfRawData for section in pe.sections), default=0)
    except Exception as e:
        print(f"Failed to extract section features: {e}")

    try:
        features['ImportsNb'] = len(pe.DIRECTORY_ENTRY_IMPORT) if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT') else 0
        features['ExportNb'] = len(pe.DIRECTORY_ENTRY_EXPORT.symbols) if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT') else 0
    except Exception as e:
        print(f"Failed to extract import/export features: {e}")

    try:
        features['ResourcesNb'] = len(pe.DIRECTORY_ENTRY_RESOURCE.entries) if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE') else 0
        resource_sizes = [(entry.sizeof_raw_data, calculate_entropy(entry.data)) for entry in pe.DIRECTORY_ENTRY_RESOURCE.entries if hasattr(entry, 'data')]
        features['ResourcesMinEntropy'] = min(entropy for size, entropy in resource_sizes) if resource_sizes else 0
        features['ResourcesMaxEntropy'] = max(entropy for size, entropy in resource_sizes) if resource_sizes else 0
        features['ResourcesMinSize'] = min(size for size, entropy in resource_sizes) if resource_sizes else 0
    except Exception as e:
        print(f"Failed to extract resource features: {e}")

    try:
        features['VersionInformationSize'] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[15].Size
    except Exception as e:
        print(f"Failed to extract data directory features: {e}")

    return features

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/predict', methods=['POST'])
def predict():
    # Check if a file was uploaded
    if 'file' not in request.files:
        return jsonify({'error': 'No file uploaded'}), 400

    file = request.files['file']

    # Clean the filename to remove spaces and special characters
    filename = re.sub(r'\W+', '', file.filename)
    filepath = 'temp_upload/' + filename

    try:
        file.save(filepath)
    except OSError as e:
        return jsonify({'error': f'Failed to save file: {e}'}), 500

    features = extract_features(filepath)  # Pass the file path to extract_features

    # Convert features to DataFrame
    df = pd.DataFrame([features])

    # Standardize the features using StandardScaler
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df)

    # Make prediction
    prediction = model.predict(df_scaled)[0]

    return render_template('result.html', prediction=prediction, features=features)

if __name__ == '__main__':
    app.run(debug=True, port=5001)
